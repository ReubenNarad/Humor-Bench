# Humor-Bench Project Context

## 1. Project Goal

Humor-Bench is a research project designed to benchmark the humor understanding capabilities of AI models, specifically Large Language Models (LLMs). The primary goal is to evaluate how well different models can identify and explain the core comedic device or concept in humorous content, using New Yorker Cartoon Caption Contest examples as the testbed.

## 2. Core Task

The benchmark requires an AI model (the "explainer") to:
1.  Receive a text description of a New Yorker cartoon image.
2.  Receive the corresponding winning caption for that cartoon.
3.  Generate a concise explanation (under 200 words) that identifies the "material substance" or key comedic element of the joke formed by the image and caption.

## 3. Dataset

The primary input dataset (`comprehensive_annotations.csv`) contains:
*   `idx`: Unique identifier for each data point.
*   `cartoonstock_id`, `contest_number`: Identifiers linking to the original cartoon source.
*   `description`: A textual description of the cartoon's visual elements. The benchmark appears to rely on these descriptions rather than actual image files.
*   `caption`: The winning caption associated with the cartoon.
*   `element`: A ground truth annotation describing the specific humor element, comedic device, or concept that the explainer model's output should ideally capture. These seem to be manually created annotations that identify the specific comedic device or humor mechanism at work in each cartoon-caption pair.

Data sources mentioned in documentation:
* New Yorker Caption Contest data via NextML
* Hugging Face `jmhessel/newyorker_caption_contest` dataset
* CartoonStock

## 4. Evaluation Methodology

Evaluation is performed using an LLM-based autograder:
*   **Explainer Model**: The LLM being benchmarked (e.g., GPT-4o, Gemini 1.5 Pro, Llama 3, Claude 3.7 Sonnet). Configured via `explainer.py`.
*   **Autograder Model**: A separate, typically powerful LLM (e.g., GPT-4o) tasked with judging the explainer's output. Configured via `autograder.py`.
*   **Process**:
    1.  The explainer model generates an explanation for a given cartoon description and caption (`explainer.py`, using `prompts.explainer_prompt`).
    2.  The autograder model receives the original description, caption, the explainer's generated explanation, and the ground truth `element` annotation (`autograder.py`, using `prompts.autograder_prompt`).
    3.  The autograder outputs a `PASS` or `FAIL` judgment based on whether the explanation *explicitly covers* the anticipated point defined in the `element` annotation, along with its reasoning.
*   **Metric**: The primary metric is the **PASS rate** - the percentage of explanations deemed `PASS` by the autograder. Cost analysis (based on token usage and `model_data/model_prices.json`) is also performed.

## 5. Autograder Validation

The reliability of the LLM autograder itself is assessed separately:
*   Scripts like `autograder_eval.py` and `autograder_eval_analysis.py` are used to evaluate the autograder against human judgments.
*   Human judgments are stored in CSV files in the `rubric/` directory (e.g., `rubric.csv`, `gpt_4o_rubric.csv`, `claude_rubric.csv`, `gemini_2.5_pro_rubric.csv`, and `llama_4_maverick_rubric.csv`), representing human PASS/FAIL labels for explanations generated by different models.
*   These labeled examples are created using a Gradio application in `rubric_annotator.py` that allows human annotators to view explanations and assign PASS/FAIL judgments.
*   The `README` mentions evaluating the autograder (GPT-4o) against 400 human judgments (PASS/FAIL labels for explanations from 4 different models across 100 elements), achieving ~87% accuracy.
*   The validation found the autograder has a higher False Positive Rate (more lenient) than False Negative Rate, meaning it's more likely to pass subpar explanations than fail good ones.

## 6. Code Structure & Execution

*   **Main Script**: `main_benchmark.py` orchestrates the process. It reads `comprehensive_annotations.csv`, initializes `ExplainerClient` and `AutograderClient` with specified models, processes rows asynchronously using workers, calculates costs, saves results to `runs/`, and performs basic analysis.
*   **Clients**: `explainer.py` and `autograder.py` contain classes (`ExplainerClient`, `AutograderClient`) to interact with various LLM APIs (OpenAI, Gemini, Claude, Together, XAI) using API keys from environment variables (`dotenv`). They handle prompt formatting, API calls, and response parsing.
*   **Prompts**: `prompts.py` defines the specific instructions given to the explainer and autograder LLMs.
*   **Annotation Tool**: `rubric_annotator.py` provides a Gradio interface for humans to annotate model-generated explanations with PASS/FAIL judgments.
*   **Evaluation**: `autograder_eval.py` compares autograder judgments against human judgments stored in the rubric files.
*   **Data**:
    *   `comprehensive_annotations.csv`: Input data with descriptions, captions, and ground truth elements.
    *   `model_data/model_prices.json`: Pricing info for various LLMs used in cost calculation.
    *   `rubric/*.csv`: Human-annotated judgments for model-generated explanations.
    *   `runs/`: Output directory for benchmark results (CSVs).
*   **Utilities**: `utils.py` contains helper functions, including the `MessageChain` class for API interactions.
*   **Analysis/Reporting**: `benchmark_results.py`, `html_report_generator.py`, `analysis/` directory contain further analysis and reporting tools. `docs/` might contain generated documentation or reports. `rubric/` might contain details related to the humor elements or evaluation criteria.
*   **Scripts**: `.sh` files (`benchmark.sh`, `run_eval.sh`, etc.) likely automate running the Python scripts with different parameters.

## 7. Key Research Aspects (Potential Paper Content)

*   **Novelty**: Introduction of Humor-Bench as a benchmark for nuanced humor understanding (identifying specific comedic devices).
*   **Dataset**: Description of the curated dataset, annotation process for descriptions and humor elements.
*   **Methodology**: Detailing the LLM-based autograding approach. Discussion of the prompt design for both explanation generation and autograding.
*   **Autograder Validation**: Presenting the evaluation of the autograder against human judgments, discussing its accuracy, biases (FPR/FNR), and implications for benchmark reliability.
*   **Model Performance**: Comparative analysis of different LLMs as "explainers" based on PASS rate and cost. Correlation analysis (e.g., cost vs. accuracy plot mentioned in `README`).
*   **Qualitative Analysis**: Examples of successful/failed explanations and autograder judgments. Analysis of *types* of humor elements models struggle with.
*   **Economic Analysis**: Cost-performance tradeoffs based on token usage tracking and model pricing information from `model_data/model_prices.json`.

## 8. Practical Application & Impact

*   **LLM Development**: Insights gained could inform improvements in LLMs' ability to understand humor and contextual nuance.
*   **Content Creation**: Potential applications in automated content moderation, joke generation, and creative writing assistance.
*   **Cultural Understanding**: Assessment of how well different models grasp culturally-specific humor elements.

## 9. Future Development

Potential directions for expanding the benchmark:
*   Include more diverse humor sources beyond New Yorker cartoons.
*   Develop metrics for specific dimensions of humor understanding (e.g., wordplay recognition, cultural reference identification).
*   Compare autograder performance across different LLMs to identify the most reliable evaluation model.
*   Explore few-shot prompting or fine-tuning to improve explainer performance.

---
*Self-Correction: Added Clarification 3 regarding human judgment location.*
