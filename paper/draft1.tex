% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[review]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{Instructions for *ACL Proceedings}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a separate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

\author{First Author \\
  Affiliation / Address line 1 \\
  Affiliation / Address line 2 \\
  Affiliation / Address line 3 \\
  \texttt{email@domain} \\\And
  Second Author \\
  Affiliation / Address line 1 \\
  Affiliation / Address line 2 \\
  Affiliation / Address line 3 \\
  \texttt{email@domain} \\}

%\author{
%  \textbf{First Author\textsuperscript{1}},
%  \textbf{Second Author\textsuperscript{1,2}},
%  \textbf{Third T. Author\textsuperscript{1}},
%  \textbf{Fourth Author\textsuperscript{1}},
%\\
%  \textbf{Fifth Author\textsuperscript{1,2}},
%  \textbf{Sixth Author\textsuperscript{1}},
%  \textbf{Seventh Author\textsuperscript{1}},
%  \textbf{Eighth Author \textsuperscript{1,2,3,4}},
%\\
%  \textbf{Ninth Author\textsuperscript{1}},
%  \textbf{Tenth Author\textsuperscript{1}},
%  \textbf{Eleventh E. Author\textsuperscript{1,2,3,4,5}},
%  \textbf{Twelfth Author\textsuperscript{1}},
%\\
%  \textbf{Thirteenth Author\textsuperscript{3}},
%  \textbf{Fourteenth F. Author\textsuperscript{2,4}},
%  \textbf{Fifteenth Author\textsuperscript{1}},
%  \textbf{Sixteenth Author\textsuperscript{1}},
%\\
%  \textbf{Seventeenth S. Author\textsuperscript{4,5}},
%  \textbf{Eighteenth Author\textsuperscript{3,4}},
%  \textbf{Nineteenth N. Author\textsuperscript{2,5}},
%  \textbf{Twentieth Author\textsuperscript{1}}
%\\
%\\
%  \textsuperscript{1}Affiliation 1,
%  \textsuperscript{2}Affiliation 2,
%  \textsuperscript{3}Affiliation 3,
%  \textsuperscript{4}Affiliation 4,
%  \textsuperscript{5}Affiliation 5
%\\
%  \small{
%    \textbf{Correspondence:} \href{mailto:email@domain}{email@domain}
%  }
%}

\begin{document}
\maketitle
\begin{abstract}
This document is a supplement to the general instructions for *ACL authors. It contains instructions for using the \LaTeX{} style files for ACL conferences.
The document itself conforms to its own specifications, and is therefore an example of what your manuscript should look like.
These instructions should be used both for papers submitted for review and for final versions of accepted papers.
\end{abstract}

\section{Introduction}
\begin{enumerate}
\item Humor comprehension represents a particularly challenging task for Large Language Models (LLMs), requiring sophisticated understanding of context, culture, and implicit connections.

\item Existing humor benchmarks conflate two distinct challenges: subjective appreciation (finding something funny) and objective comprehension (understanding why something is intended to be humorous). Our work focuses exclusively on the latter, creating a benchmark that isolates and evaluates humor comprehension abilities.

\item Our dataset draws from New Yorker cartoon caption contests and cartoonstock.com, sources known for sophisticated humor requiring cultural knowledge, understanding of tropes, and nuanced storytelling. We carefully selected \textcolor{red}{300} New Yorker cartoons and captions, plus \textcolor{red}{100} best-selling cartoons from cartoonstock.com, specifically choosing examples that feature subtle or complex humor elements.

\item HumorBench tests an LLM's ability to identify and reason about connections between visual elements, textual captions, and external knowledge. Success requires recognizing references, wordplay, cultural tropes, and shared human experiences that underlie the humor. \textcolor{red}{We categorize these comprehension elements into distinct types (references, subversions, absurdities, etc.) to enable fine-grained analysis.}

\item For each (cartoon, caption) pair, we provide detailed annotations of all elements essential for complete humor comprehension, creating a ground truth for evaluation.

\item Figure 1 illustrates our approach using a representative example. The left column ("Comprehension Elements") lists the specific contextual knowledge and connections required to understand the humor (e.g., "recognition that the figure represents Death," "understanding of chess as a game of strategy"). The right column ("Subjectivity Factors") shows potential reasons why a reader might fail to find the humor amusing despite comprehending it (e.g., "reader does not find references to mortality funny," "reader has no personal interest in chess"). The figure visually demonstrates how humor comprehension requires connecting elements within the cartoon to external concepts and knowledge.

The figure should also include something that shows the ``connection" to external concepts. I can draw a diagram once we have a cartoon and caption selected.

\item We release two benchmark versions: HumorBench and HumorBench-hard (HBH). While frontier reasoning models perform reasonably well on the standard HumorBench, open-source models still struggle significantly. On HumorBench-hard, which features more complex examples, no current LLM exceeds \textcolor{red}{20\%} accuracy. \textcolor{red}{Based on our initial results, we'll determine whether HBH should be a distinct subset or if we should reorganize the entire dataset into difficulty-based divisions.}

\item HumorBench serves as a novel evaluation framework that reveals specific gaps in LLM comprehension abilities, particularly in making the conceptual leaps required to connect disparate elements and recognize implicit meaningâ€”skills essential not just for humor but for advanced natural language understanding.

\end{enumerate}

\section{Related Work}

\section{HumorBench}

\subsection{Why Another Humor Benchmark?}
Existing works on NYCC, our goal and differentiation. Expanded version of what's dicussed in related work.

\begin{enumerate}
    \item Discuss what is the New Yorker style humor. The challenges.

    \item Previous work focus on ranking, explanation and generation. Discuss what each of them does. They all conflate objective and subjective factors. Discuss especially how our explanation task is different from the explanation task in Hessel. You can reference the ``our task" section below.
\end{enumerate}

\subsection{Our Task}
We want the model to explain humor, but we judge them only by what are the objective comprehension elements involved that are necessary to fully understand the caption.

Unlike the explanation task in Hessel, we don't judge the model's explanation of what elements may likely cause the specific new yorker audience to laugh at.

Give the specific explanation task, where we ask the model to generate an explanation of the joke, and we expect the model's output to cover all of the annotated elements.

\subsection{Autograder Construction}
We want to mention the accuracy of the autograder. Reference previous OpenAI and other papers that struggle with setting up a perfect autograder. Our FNR is high and FPR is low, which means we have a rather lenient autograder, so the performance reported should be seen as an upper bound of the LLM's capabilities.

\textcolor{red}{It would nice to also include an ablation study in the appendix showing the autograder's accuracies and FPR/FNR are not significantly different when judging different model's explanations. This way, we don't have to rebut the potential reviewer's comment about "GPT-4o may be biased to prefer OpenAI models", etc.}

\section{Dataset Curation}
\subsection{Cartoon Caption Selection}
Captions ranked higher in crowdsourcing are not as complicated as the ones chosen as finalist by the editors.

\subsection{Comprehension Element Annotation}
Reuben can include how he fixes the elements that are unclear. We should also mention that these are expert annotated and checked.

\section{Experiments}
Mention any additonal experiment settings including prompts we used, APIs we used, etc.

\subsection{Main Results}
I think Reuben has quite some interesting findings. We should just add discussions.

\subsection{Comparison and Correlations to Other Benchmarks}
Plot with x axis being MMLU and LLM-arena, and y axis being humorbench or HBH.

\subsection{Analysis of HBH}
What are the failure cases? What's in HBH? Categorize these hard instances and give some quantitative metrics.

\subsection{[Optional] Human-AI collaborative explanation}
Human may give some hints around what AI should focus on, but may be too lazy to write everything themselves. We can generate hints by using LLMs when given the ground truth elements. Essentially, the hints help LLMs focusing on the right directions. \textcolor{red}{May see gaps between reasoning models and non-reasoning models close? Do models still fail on this task? Is there anything deeper that is causing the models to fail on this task than ``need better reasoning"?}

\section{Conclusion and Future Work}
I am not sure what the RL argument is here, but Reuben can try to flesh it out a little? Not sure what serves as rewards. Are the annotated elements process data? Or are they the final answer? Are we doing RL to train LLMs to better drawing connections? 


\newpage
\section*{Limitations}

Since December 2023, a "Limitations" section has been required for all papers submitted to ACL Rolling Review (ARR). This section should be placed at the end of the paper, before the references. The "Limitations" section (along with, optionally, a section for ethical considerations) may be up to one page and will not count toward the final page limit. Note that these files may be used by venues that do not rely on ARR so it is recommended to verify the requirement of a "Limitations" section and other criteria with the venue in question.

\section*{Acknowledgments}

This document has been adapted
by Steven Bethard, Ryan Cotterell and Rui Yan
from the instructions for earlier ACL and NAACL proceedings, including those for
ACL 2019 by Douwe Kiela and Ivan Vuli\'{c},
NAACL 2019 by Stephanie Lukin and Alla Roskovskaya,
ACL 2018 by Shay Cohen, Kevin Gimpel, and Wei Lu,
NAACL 2018 by Margaret Mitchell and Stephanie Lukin,
Bib\TeX{} suggestions for (NA)ACL 2017/2018 from Jason Eisner,
ACL 2017 by Dan Gildea and Min-Yen Kan,
NAACL 2017 by Margaret Mitchell,
ACL 2012 by Maggie Li and Michael White,
ACL 2010 by Jing-Shin Chang and Philipp Koehn,
ACL 2008 by Johanna D. Moore, Simone Teufel, James Allan, and Sadaoki Furui,
ACL 2005 by Hwee Tou Ng and Kemal Oflazer,
ACL 2002 by Eugene Charniak and Dekang Lin,
and earlier ACL and EACL formats written by several people, including
John Chen, Henry S. Thompson and Donald Walker.
Additional elements were taken from the formatting instructions of the \emph{International Joint Conference on Artificial Intelligence} and the \emph{Conference on Computer Vision and Pattern Recognition}.

% Bibliography entries for the entire Anthology, followed by custom entries
%\bibliography{anthology,custom}
% Custom bibliography entries only
\bibliography{custom}

\appendix

\section{Example Appendix}
\label{sec:appendix}

This is an appendix.

\end{document}
